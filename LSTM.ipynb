{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"L9DSfH_mXFAU","colab_type":"text"},"source":["##Mounting Google Drive to Notebook##"]},{"cell_type":"code","metadata":{"id":"ZTw5js3EAPsB","colab_type":"code","outputId":"3006e413-7424-4453-d04e-b013c83d4592","executionInfo":{"status":"error","timestamp":1581638165845,"user_tz":300,"elapsed":6296,"user":{"displayName":"Nicholas Miller","photoUrl":"","userId":"10828150341597191663"}},"colab":{"base_uri":"https://localhost:8080/","height":507}},"source":["# mounting for google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-80bea5481fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"K2HJQFMzW9sR","colab_type":"text"},"source":["##Import Functions##"]},{"cell_type":"code","metadata":{"id":"7hr4f15DDrLL","colab_type":"code","colab":{}},"source":["# helper and visualization imports \n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n","\n","# embedding imports\n","from gensim.models import KeyedVectors\n","\n","# model imports\n","from keras import backend as K\n","from keras.models import Model, Sequential\n","from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate, Activation\n","from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.preprocessing import text, sequence\n","from keras.activations import softsign\n","from tensorflow.contrib.tpu import keras_to_tpu_model\n","\n","# metrics imports\n","from math import sqrt\n","from sklearn.metrics import confusion_matrix\n","\n","# preprocessing imports \n","import pylab\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","from sklearn.utils import shuffle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI81w2MqXLYS","colab_type":"text"},"source":["##Constant Definitions##"]},{"cell_type":"code","metadata":{"id":"b6FOnJfoEEqe","colab_type":"code","colab":{}},"source":["# imports for the embedding files\n","EMBEDDING_FILES = [\n","    '/content/gdrive/My Drive/crawl-300d-2M.gensim',\n","    '/content/gdrive/My Drive/glove.840B.300d.gensim'\n","]\n","\n","# LSTM model parameters\n","BATCH_SIZE = 512\n","LSTM_UNITS = 128\n","DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n","EPOCHS = 4\n","MAX_LEN = 220\n","\n","# LSTM training and testing attributes\n","IDENTITY_COLUMNS = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n","                    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n","AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n","TEXT_COLUMN = 'comment_text'\n","TARGET_COLUMN = 'target'\n","RATING_COLUMN = 'rating'\n","TR_COLUMN = ['target', 'rating']\n","RATING = ['approved', 'rejected']\n","\n","# preprocessing constants\n","CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n","\n","CONTRACTIONS_DICT = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n","                     \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n","                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n","                     \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \n","                     \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n","                     \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n","                     \"how'll\": \"how will\", \"how's\": \"how has\", \"I'd\": \"I had\", \"I'd've\": \"I would have\",\n","                     \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\",\n","                     \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n","                     \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n","                     \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n","                     \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n","                     \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n","                     \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n","                     \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she had\",\n","                     \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n","                     \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n","                     \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\",\n","                     \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n","                     \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n","                     \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n","                     \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n","                     \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n","                     \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n","                     \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n","                     \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n","                     \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n","                     \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n","                     \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n","                     \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n","                     \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \n","                     \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n","                     \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n","                     \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","SPECIAL_CHARS = {'~',':','+','[','\\\\','@','^','{','%','(','-','\"','*','|',',','&'\n","                ,'<','}','.','_','=',']','!','>',';','?','~','#','$',')','/','∞'\n","                ,'θ','÷','α','•','à','−','β','∅','³','π','‘','₹','´','°','£','€'}\n","\n","#STOP_WORDS = set(stopwords.words('english')) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MY2Gy-5tXTyy","colab_type":"text"},"source":["##Function Definitions##"]},{"cell_type":"code","metadata":{"id":"Yi0eNuGjEVw9","colab_type":"code","colab":{}},"source":["#----------------------------------LSTM Embedding/Model Functions------------------------------------\n","# function to build the embedded matrix \n","def build_matrix(word_index, path):\n","    embedding_index = KeyedVectors.load(path, mmap='r')\n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    for word, i in word_index.items():\n","        for candidate in [word, word.lower()]:\n","            if candidate in embedding_index:\n","                embedding_matrix[i] = embedding_index[candidate]\n","                break\n","    return embedding_matrix\n","    \n","# function to build the LSTM model\n","def build_model(embedding_matrix, num_aux_targets):\n","    words = Input(shape=(None,))\n","    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n","    x = SpatialDropout1D(0.2)(x)\n","    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n","    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n","\n","    hidden = concatenate([\n","        GlobalMaxPooling1D()(x),\n","        GlobalAveragePooling1D()(x),\n","    ])\n","    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    result = Dense(1, activation='sigmoid')(hidden)\n","    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n","    \n","    model = Model(inputs=words, outputs=[result, aux_result])\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model\n","\n","\n","#----------------------------------Rating Conversion Function for NN-----------------------------------\n","# function used to convert ratings in to binary digitd for neural network training\n","def rating_conversion(rating_list):\n","  new_rating = []\n","  for rating in rating_list:\n","    if rating == 'approved': new_rating.append(1)\n","    else: new_rating.append(0)\n","  return new_rating\n","\n","\n","#---------------------------------------Keras Function Metrics------------------------------------------\n","# NN recall metric \n","def recall_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","# NN precision metric \n","def precision_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","# NN F1 metric \n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","# NN RMSE metric \n","def rmse_m(y_true, y_pred):\n","    return sqrt(mean_squared_error(y_true, y_pred))\n","\n","\n","#---------------------------------------Regression Analysis--------------------------------------------\n","# This is an average of the square differences between the truth and prediction\n","def mean_squared_error(truth, pred):\n","  a = (pred - truth) * (pred - truth)\n","  return np.average(a)\n","\n","# This is an square root of the average of the square differences between the truth and prediction\n","def root_mean_squared_error(truth, pred):\n","  return sqrt(mean_squared_error(pred, truth))\n","\n","# This is an average in the differences in the prediction and truth \n","def mean_absolute_error(truth, pred):\n","  return np.average(np.absolute(pred - truth))\n","\n","# This is the MSE relative to the variance of the truth\n","def relative_mean_squared_error(truth, pred):\n","  return mean_squared_error(truth, pred) / np.var(truth)\n","\n","# This is an accuracy measure \n","def r2_score(truth, pred):\n","  return 1 - relative_mean_squared_error(truth, pred)\n","\n","\n","#---------------------------------------Preprocessing Functions-----------------------------------------\n","# function to remove special characters \n","def special_char_prune(text):\n","    filtered_chars = [c for c in text if not c in SPECIAL_CHARS]\n","    filtered_text = ''.join(filtered_chars)\n","    return filtered_text\n","\n","# function to map contractions \n","def contraction_pruning(text):\n","    no_special = special_char_prune(text)\n","    word_tokens = no_special.split()\n","    filtered_sentence = \"\"\n","    for token in word_tokens:\n","        word = token.lower()\n","        if word in CONTRACTIONS_DICT: filtered_sentence = filtered_sentence + \" \" + CONTRACTIONS_DICT[word]\n","        else: filtered_sentence = filtered_sentence + \" \" + word\n","    return filtered_sentence.strip()\n","\n","# function to remove stopwords\n","def stop_word_pruning(text):\n","    word_tokens = word_tokenize(text) \n","    no_stop = [w for w in word_tokens if not w in STOP_WORDS]\n","    filtered_sentence = ' '.join(no_stop)\n","    return filtered_sentence    \n","\n","# this creates bins for target scores \n","def target_round(a):\n","    if (a <= 0.33):\n","        c = 0.0\n","    elif (a <= 0.66):\n","        c = 0.5\n","    else: c = 1.0\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKd_z5K5XXIX","colab_type":"text"},"source":["##Testing and Training Set Building##"]},{"cell_type":"code","metadata":{"id":"OrbX1sjOQDwz","colab_type":"code","colab":{}},"source":["# read file extension\n","file_name = 'file_name' # name of file in google drive \n","file_path = '/content/gdrive/My Drive/' + file_name + '.csv'\n","\n","x_split = 1700000\n","y_split = 104874\n","\n","# These are the slips for the data\n","# 1700000 104874\n","# 90000 25000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yczC5KyEtcV","colab_type":"code","colab":{}},"source":["# This is the testing and traing data I use for the simple NN\n","# imports for the training and testing sets \n","df = pd.read_csv(file_path)\n","train_NN_df = df.iloc[:x_split, :]\n","\n","# training data for the neural network \n","train_x_NN = np.asarray(train_NN_df[TARGET_COLUMN].tolist(), dtype=np.float32).reshape(x_split)\n","train_y_NN = np.asarray(rating_conversion(train_NN_df[RATING_COLUMN].tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6JCLjDgiEv8h","colab_type":"code","outputId":"7453ab1c-5c7a-4906-df1f-cc19f765e429","executionInfo":{"status":"ok","timestamp":1575655057946,"user_tz":300,"elapsed":106786,"user":{"displayName":"Nicholas Miller","photoUrl":"","userId":"10828150341597191663"}},"colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["# This is the testing and traing data I use for the LSTM \n","# Import and split data into test and train\n","df = pd.read_csv(file_path)\n","train_df = df.iloc[:x_split, :]\n","test_df = df.iloc[x_split:, :]\n","\n","# Create training splits for text and auxilary columns \n","x_train = train_df[TEXT_COLUMN].astype(str)\n","y_train = train_df[TARGET_COLUMN].values\n","y_aux_train = train_df[AUX_COLUMNS].values\n","x_test = test_df[TEXT_COLUMN].astype(str)\n","\n","for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n","    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n","\n","# tokenize text to remove bad characters\n","tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\n","tokenizer.fit_on_texts(list(x_train) + list(x_test))\n","\n","# resequence characters back into text \n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n","x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n","\n","# initialize the sample weights of the LSTM \n","sample_weights = np.ones(len(x_train), dtype=np.float32)\n","sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n","sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n","sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n","sample_weights /= sample_weights.mean()\n","\n","# building of the embedding matrix \n","embedding_matrix = np.concatenate(\n","    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"c9XS-Y28Pefu","colab_type":"code","colab":{}},"source":["# truth sets for the LSTM and NN \n","df = pd.read_csv(file_path)\n","test_app_df = df.iloc[x_split:, :]\n","\n","LSTM_TRUTH = np.asarray(test_app_df[TARGET_COLUMN].tolist()).reshape(y_split)\n","NN_TRUTH = np.asarray(rating_conversion(test_app_df[RATING_COLUMN].tolist())).reshape(y_split)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWT6JvWqXeAQ","colab_type":"text"},"source":["##Model Training##"]},{"cell_type":"code","metadata":{"id":"bBArwaQpljzS","colab_type":"code","outputId":"772c1104-28df-4fb3-edb1-eee59d0639a0","executionInfo":{"status":"ok","timestamp":1575655222664,"user_tz":300,"elapsed":6055,"user":{"displayName":"Nicholas Miller","photoUrl":"","userId":"10828150341597191663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# LSTM Model summary\n","model = build_model(embedding_matrix, y_aux_train.shape[-1])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, None, 600)    66556800    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","spatial_dropout1d_1 (SpatialDro (None, None, 600)    0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, None, 256)    747520      spatial_dropout1d_1[0][0]        \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, None, 256)    395264      bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_1[0][0]     \n","                                                                 global_average_pooling1d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          262656      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 512)          0           concatenate_1[0][0]              \n","                                                                 dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 512)          262656      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 512)          0           add_1[0][0]                      \n","                                                                 dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            513         add_2[0][0]                      \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 6)            3078        add_2[0][0]                      \n","==================================================================================================\n","Total params: 68,228,487\n","Trainable params: 1,671,687\n","Non-trainable params: 66,556,800\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1lJoituXLx9v","colab_type":"code","outputId":"53c78687-f3e8-4907-b80e-da0df1ce0a6d","executionInfo":{"status":"ok","timestamp":1575655399679,"user_tz":300,"elapsed":178340,"user":{"displayName":"Nicholas Miller","photoUrl":"","userId":"10828150341597191663"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["# LSTM Model\n","checkpoint_predictions = []\n","weights = []\n","\n","model = build_model(embedding_matrix, y_aux_train.shape[-1])\n","for global_epoch in range(EPOCHS):\n","  model.fit(x_train, [y_train, y_aux_train], batch_size=BATCH_SIZE, epochs=1,\n","              verbose=2, sample_weight=[sample_weights.values, np.ones_like(sample_weights)])\n","  checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n","  weights.append(2 ** global_epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/1\n"," - 44s - loss: 0.7292 - dense_7_loss: 0.5574 - dense_8_loss: 0.1719 - dense_7_acc: 0.5859 - dense_8_acc: 0.7973\n","Epoch 1/1\n"," - 39s - loss: 0.6587 - dense_7_loss: 0.5135 - dense_8_loss: 0.1453 - dense_7_acc: 0.6057 - dense_8_acc: 0.8003\n","Epoch 1/1\n"," - 39s - loss: 0.6453 - dense_7_loss: 0.5043 - dense_8_loss: 0.1409 - dense_7_acc: 0.6085 - dense_8_acc: 0.8004\n","Epoch 1/1\n"," - 39s - loss: 0.6350 - dense_7_loss: 0.4969 - dense_8_loss: 0.1382 - dense_7_acc: 0.6095 - dense_8_acc: 0.8005\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohrUk7llc-Tg","colab_type":"code","colab":{}},"source":["# NN model\n","model_NN = Sequential()\n","model_NN.add(Dense(1, input_dim=1, activation='sigmoid')) \n","\n","# compile NN model\n","model_NN.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc', f1_m, precision_m, recall_m]) \n","model_NN.fit(train_x_NN, train_y_NN, batch_size=64, nb_epoch=5, verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqOFu-_hXjUU","colab_type":"text"},"source":["##Model Evaluation##"]},{"cell_type":"code","metadata":{"id":"FgvfeBXQBFRk","colab_type":"code","colab":{}},"source":["## LSTM Predictions for submission \n","predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","submission = pd.DataFrame.from_dict({\n","    'id': test_df.id,\n","    'prediction': predictions\n","})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMpRVnO8d5OM","colab_type":"code","colab":{}},"source":["# LSTM predictions\n","lstm_pred = submission.sort_index()\n","lstm_pred = np.array(lstm_pred['prediction'].tolist())\n","\n","# LSTM prediction metrics \n","MSE_LSTM = mean_squared_error(LSTM_TRUTH, lstm_pred)\n","RMSE_LSTM = root_mean_squared_error(LSTM_TRUTH, lstm_pred)\n","MAE_LSTM = mean_absolute_error(LSTM_TRUTH, lstm_pred)\n","rMSE_LSTM = relative_mean_squared_error(LSTM_TRUTH, lstm_pred)\n","R2_LSTM = r2_score(LSTM_TRUTH, lstm_pred)\n","\n","# print all metrics \n","print('MSE of LSTM: ' + str(MSE_LSTM) + '\\n' + \n","      'RMSE of LSTM: ' + str(RMSE_LSTM) + '\\n' + \n","      'MAE of LSTM: ' + str(MAE_LSTM) + '\\n' + \n","      'rMSE of LSTM: ' + str(rMSE_LSTM) + '\\n' + \n","      'R2 of LSTM: ' + str(R2_LSTM))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5gyAKNi3H0k","colab_type":"code","colab":{}},"source":["# visualization for QQ-Plot of LSTM predication vs. truth\n","fig = plt.figure()\n","plt.scatter(LSTM_TRUTH, lstm_pred)\n","fig.suptitle('LSTM Scatter for CM', fontsize=20)\n","plt.xlabel('LSTM TRUTH', fontsize=16)\n","plt.ylabel('LSTM Prediction', fontsize=16)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_91kakOwn0p","colab_type":"code","colab":{}},"source":["# NN preditions\n","nn_pred = model_NN.predict(lstm_pred).reshape(y_split)\n","nn_pred = np.around(nn_pred)\n","\n","# keras model pred\n","_, accuracy, f1_score, precision, recall = model_NN.evaluate(lstm_pred, NN_TRUTH, verbose=0)\n","\n","# print keras metrics\n","print('Accuracy: ' + str(accuracy) + '\\n' + \n","      'F1_score: ' + str(f1_score) + '\\n' + \n","      'Precision: ' + str(precision) + '\\n' + \n","      'Recall: ' + str(recall))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbBq_1abuqEA","colab_type":"code","colab":{}},"source":["# confusion matrix of NN predictions \n","cm = confusion_matrix(NN_TRUTH, nn_pred)\n","print(cm)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBs6RoLIcG0n","colab_type":"text"},"source":["###LSTM as Classifier"]},{"cell_type":"code","metadata":{"id":"jN2LW-axX5mm","colab_type":"code","colab":{}},"source":["LSTM_TOXIC_TRUTH = []\n","for elem in LSTM_TRUTH:\n","  if elem >= 0.5: LSTM_TOXIC_TRUTH.append(1)\n","  else: LSTM_TOXIC_TRUTH.append(0)\n","\n","LSTM_TOXIC_PRED = []\n","for elem in submission['prediction']:\n","  if elem >= 0.5: LSTM_TOXIC_PRED.append(1)\n","  else: LSTM_TOXIC_PRED.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QM_wJm0Za-eY","colab_type":"code","colab":{}},"source":["lstm_class_df = pd.DataFrame({'LSTM_TRUTH':LSTM_TOXIC_TRUTH, \n","                   'LSTM_PRED':LSTM_TOXIC_PRED})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxqXQdNGbc72","colab_type":"code","colab":{}},"source":["lstm_class_df.to_csv('/content/gdrive/My Drive/lstm_classification.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nzdsxThbW-B","colab_type":"text"},"source":["##Analysis##"]},{"cell_type":"code","metadata":{"id":"1iPThZWXbkHn","colab_type":"code","colab":{}},"source":["# Analysis of comment distribution\n","df = pd.read_csv(file_path)\n","df_analysis = df[TR_COLUMN]\n","df_group = df_analysis.groupby(RATING_COLUMN)\n","\n","# grouped based on rating\n","df1 = df_group.get_group(RATING[0])\n","df2 = df_group.get_group(RATING[1])\n","\n","# visualization for counts of rating based on target score\n","fig = plt.figure()\n","plt.hist([df1[TARGET_COLUMN], df2[TARGET_COLUMN]],bins=10, range=(0,1), stacked=True, color = ['g','r'])\n","fig.suptitle('Comment Rating Distribution', fontsize=20)\n","plt.xlabel('Comment Counts', fontsize=16)\n","plt.ylabel('Comment Target Score', fontsize=16)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6K9Y656QJgx","colab_type":"text"},"source":["###Sample Data###"]},{"cell_type":"code","metadata":{"id":"vqMnJ9JCQMJq","colab_type":"code","colab":{}},"source":["# read file extension\n","file_name_s = 'file_name' # name of file in google drive \n","file_path_s = '/content/gdrive/My Drive/' + file_name_s + '.csv'\n","\n","save_name_s = 'file 1 name'\n","\n","# load main df \n","df = pd.read_csv(file_path_s)\n","\n","# create list of target bins \n","target_list = []\n","for i in range(len(df)):\n","    target_list.append(round(df.loc[i, 'target']))\n","    if (i % 250000 == 0): print(i)\n","\n","# add new column to df\n","df['target_rounded'] = target_list\n","\n","# groupby target bin then sample \n","df_group = df.groupby('rating')\n","df_zero_sample = df_group.get_group(0.0).sample(90000)\n","df_half_sample = df_group.get_group(0.5).sample(16000)\n","df_one_sample = df_group.get_group(1.0).sample(9000)\n","\n","# combine new dfs, shuffle, and drop target bin column \n","df_combined = pd.concat([df_zero_sample, df_half_sample, df_one_sample], ignore_index=True)\n","df_shuffled = shuffle(df_combined)\n","df_final = df_shuffled.drop(columns=['target_rounded'])\n","\n","# save to csv\n","df_final.to_csv(save_name_s + '.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tg5qEBHToHiT","colab_type":"text"},"source":["###Preprocessing Data###"]},{"cell_type":"code","metadata":{"id":"Sl_GH0A8oK-S","colab_type":"code","colab":{}},"source":["# this is the name of the sampled file \n","pre_file_name = 'file name' \n","pre_path = '/content/gdrive/My Drive/' + pre_file_name + '.csv'\n","\n","# these are what the file will be saved as \n","save_name1 = 'file 1 name'\n","save_name2 = 'file 2 name'\n","\n","# load df from path \n","df1 = pd.read_csv(pre_path)\n","df2 = pd.read_csv(pre_path)\n","\n","# apply functions to comments \n","df1[TEXT_COLUMN] = df1.apply(lambda x: contraction_pruning(x[TEXT_COLUMN]), axis=1)\n","df2[TEXT_COLUMN] = df1.apply(lambda x: stop_word_pruning(x[TEXT_COLUMN]), axis=1)\n","\n","# save to csv's\n","df1.to_csv(save_name1 + '.csv', index=False)\n","df2.to_csv(save_name2 + '.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65pDpfaXXnlI","colab_type":"text"},"source":["##Save File for Submission##"]},{"cell_type":"code","metadata":{"id":"tXqm8owYHdh7","colab_type":"code","colab":{}},"source":["# submission for Kaggle competition \n","submission.to_csv('/content/gdrive/My Drive/Model_1.csv')"],"execution_count":0,"outputs":[]}]}